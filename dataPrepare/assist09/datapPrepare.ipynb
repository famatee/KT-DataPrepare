{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from EduData import get_data\n",
    "from sklearn.model_selection import KFold   # 不弄k-fold交叉验证就不用这个\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据存放base路径\n",
    "basePath='../../data/assist09/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloader, INFO http://base.ustc.edu.cn/data/ASSISTment/2009_skill_builder_data_corrected.zip is saved as ../../data/assist09/2009_skill_builder_data_corrected.zip\n",
      "downloader, INFO file existed, skipped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../../data/assist09/2009_skill_builder_data_corrected'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果还没下载数据集原始文件，可以从edudata在线下载\n",
    "get_data('assistment-2009-2010-skill',basePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hejunliang/.conda/envs/hjlTorch1/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3194: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(basePath+'2009_skill_builder_data_corrected/skill_builder_data_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['order_id', 'assignment_id', 'user_id', 'assistment_id', 'problem_id',\n",
      "       'original', 'correct', 'attempt_count', 'ms_first_response',\n",
      "       'tutor_mode', 'answer_type', 'sequence_id', 'student_class_id',\n",
      "       'position', 'type', 'base_sequence_id', 'skill_id', 'skill_name',\n",
      "       'teacher_id', 'school_id', 'hint_count', 'hint_total', 'overlap_time',\n",
      "       'template_id', 'answer_id', 'answer_text', 'first_action',\n",
      "       'bottom_hint', 'opportunity', 'opportunity_original'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己决定要用哪些列\n",
    "data = pd.read_csv(\n",
    "    basePath+'2009_skill_builder_data_corrected/skill_builder_data_corrected.csv',\n",
    "    usecols=['order_id', 'user_id', 'problem_id', 'skill_id','template_id', 'correct']\n",
    ").dropna(subset=['order_id', 'user_id', 'problem_id', 'skill_id', 'template_id','correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of problems: 17751\n"
     ]
    }
   ],
   "source": [
    "# 建立练习映射,编号从1~n\n",
    "raw_problem=data.problem_id.unique().tolist()\n",
    "raw_problem.sort()\n",
    "num_problem=len(raw_problem)\n",
    "problems={p:i+1 for i,p in enumerate(raw_problem)}\n",
    "print(\"number of problems: %d\" % num_problem)\n",
    "np.save(basePath+'map/eMap.npy',problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将problem_id替换成eMap的value\n",
    "data['problem_id']=data['problem_id'].map(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of skills: 123\n"
     ]
    }
   ],
   "source": [
    "# 建立技能映射，编号从0~n-1\n",
    "raw_question = data.skill_id.unique().tolist()\n",
    "num_skill = len(raw_question)\n",
    "skills = { p: i for i, p in enumerate(raw_question) }\n",
    "print(\"number of skills: %d\" % num_skill)\n",
    "np.save(basePath+'map/cMap.npy',skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将skill_id替换成cMap的value\n",
    "data['skill_id']=data['skill_id'].map(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        problem_id  skill_id\n",
      "146242           1        39\n",
      "191473           2        49\n",
      "112391           3        31\n",
      "191472           4        49\n",
      "112390           4        31\n",
      "...            ...       ...\n",
      "253931       17747        81\n",
      "253932       17748        81\n",
      "253942       17749        81\n",
      "253933       17750        81\n",
      "72043        17751        20\n",
      "\n",
      "[21246 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 建立练习-技能-邻接矩阵\n",
    "adj_problem_skill=np.zeros((num_problem+1,num_skill))\n",
    "single_problem_skill_pair=data.drop_duplicates(subset=['problem_id','skill_id'])[['problem_id','skill_id']].sort_values(by=['problem_id'])\n",
    "print(single_problem_skill_pair)\n",
    "for i,row in single_problem_skill_pair.iterrows():\n",
    "    adj_problem_skill[int(row['problem_id'])][int(row['skill_id'])]=1\n",
    "#保存e2c邻接矩阵\n",
    "np.save(basePath+'adj/e2cAdj.npy',adj_problem_skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of templates: 658\n"
     ]
    }
   ],
   "source": [
    "# 建立template_id映射，编号从0~n-1\n",
    "raw_template = data.template_id.unique().tolist()\n",
    "num_template = len(raw_template)\n",
    "\n",
    "templates = { p: i for i, p in enumerate(raw_template) }\n",
    "print(\"number of templates: %d\" % num_template)\n",
    "np.save(basePath+'map/tMap.npy',templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['template_id']=data['template_id'].map(templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        problem_id  template_id\n",
      "146242           1          238\n",
      "191473           2          238\n",
      "112391           3          238\n",
      "112390           4          238\n",
      "146278           5          239\n",
      "...            ...          ...\n",
      "253931       17747          464\n",
      "253932       17748          467\n",
      "253942       17749          467\n",
      "253933       17750          467\n",
      "72043        17751          179\n",
      "\n",
      "[17751 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 建立练习-template-邻接矩阵\n",
    "adj_problem_template=np.zeros((num_problem+1,num_template))\n",
    "single_problem_template_pair=data.drop_duplicates(subset=['problem_id','template_id'])[['problem_id','template_id']].sort_values(by=['problem_id'])\n",
    "print(single_problem_template_pair)\n",
    "for i,row in single_problem_template_pair.iterrows():\n",
    "    adj_problem_template[int(row['problem_id'])][int(row['template_id'])]=1\n",
    "#保存e2c邻接矩阵\n",
    "np.save(basePath+'adj/e2tAdj.npy',adj_problem_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parse student sequence:\t:   0%|          | 0/4163 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parse student sequence:\t: 100%|██████████| 4163/4163 [00:02<00:00, 1396.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总的学生人数： 4163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 将每个学生的答题序列分好块\n",
    "def parse_all_seq(students):\n",
    "    all_sequences = []\n",
    "    for student_id in tqdm.tqdm(students, 'parse student sequence:\\t'):\n",
    "        student_sequence = parse_student_seq(data[data.user_id == student_id])\n",
    "        all_sequences.extend([student_sequence])\n",
    "    return all_sequences\n",
    "\n",
    "\n",
    "def parse_student_seq(student):\n",
    "    seq = student.sort_values('order_id')\n",
    "    return seq['problem_id'].values,seq['correct'].values\n",
    "\n",
    "data=data.drop_duplicates(subset=['order_id'])  # 根据要求看要不要去掉重复记录\n",
    "sequences = parse_all_seq(sorted(data.user_id.unique()))\n",
    "print('总的学生人数：',len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'list'>\n",
      "(array([12668, 12692, 12685, 12704, 12705, 12700, 12708,  2993,  3182,\n",
      "        2977,  3173,  3168, 12032, 12242, 12231, 11732, 12213, 11712,\n",
      "       11715]), array([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(type(sequences[0]))\n",
    "print(type(sequences))\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1236135/1491583891.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  allFoldSeq.append((np.array(sequences)[train_index], np.array(sequences)[test_index]))\n"
     ]
    }
   ],
   "source": [
    "# k-fold交叉校验\n",
    "kf = KFold(n_splits=5,shuffle=True)\n",
    "allFoldSeq=[]\n",
    "\n",
    "for train_index,test_index in kf.split(sequences):\n",
    "    allFoldSeq.append((np.array(sequences)[train_index], np.array(sequences)[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hejunliang/.conda/envs/hjlTorch1/lib/python3.8/site-packages/numpy/lib/npyio.py:521: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    }
   ],
   "source": [
    "np.save(basePath+'raw/allFoldSeq.npy',allFoldSeq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 常见的一种存储方式，还未分割/padding到统一长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequences2tl(sequences, trgpath):\n",
    "    with open(trgpath, 'a', encoding='utf8') as f:\n",
    "        for seq in tqdm.tqdm(sequences, 'write into file: '):\n",
    "            questions, answers = seq\n",
    "            seq_len = len(questions)\n",
    "            f.write(str(seq_len) + '\\n')\n",
    "            f.write(','.join([str(q) for q in questions]) + '\\n')\n",
    "            f.write(','.join([str(a) for a in answers]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面是我自己的存储方式，请根据自己要求存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitToMaxStep(sequences,maxstep):\n",
    "    e_data=[]\n",
    "    a_data=[]\n",
    "    for e_features,a in tqdm.tqdm(sequences, 'splitting into MaxStep: '):\n",
    "        \n",
    "        length=e_features.shape[0]\n",
    "        slices = length//maxstep + (1 if length%maxstep > 0 else 0)\n",
    "        for i in range(slices):\n",
    "            e_temp = np.zeros(shape=[maxstep,1])\n",
    "            a_temp = np.zeros(shape=[maxstep,1])\n",
    "            if length>0:\n",
    "                if length>=maxstep:\n",
    "                    l=maxstep\n",
    "                else:\n",
    "                    l=length\n",
    "                for j in range(l):\n",
    "                    e_temp[j]=e_features[i*maxstep+j]\n",
    "                    a_temp[j]=a[i*maxstep+j]\n",
    "                length = length - maxstep\n",
    "            e_data.append(e_temp)\n",
    "            a_data.append(a_temp)\n",
    "    \n",
    "    return np.concatenate((np.array(e_data).astype(float),np.array(a_data).astype(float)),axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splitting into MaxStep:   0%|          | 0/3330 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splitting into MaxStep: 100%|██████████| 3330/3330 [00:00<00:00, 17034.27it/s]\n",
      "splitting into MaxStep: 100%|██████████| 833/833 [00:00<00:00, 17605.45it/s]\n",
      "splitting into MaxStep: 100%|██████████| 3330/3330 [00:00<00:00, 17624.95it/s]\n",
      "splitting into MaxStep: 100%|██████████| 833/833 [00:00<00:00, 15997.80it/s]\n",
      "splitting into MaxStep: 100%|██████████| 3330/3330 [00:00<00:00, 16815.76it/s]\n",
      "splitting into MaxStep: 100%|██████████| 833/833 [00:00<00:00, 16580.64it/s]\n",
      "splitting into MaxStep: 100%|██████████| 3331/3331 [00:00<00:00, 17631.40it/s]\n",
      "splitting into MaxStep: 100%|██████████| 832/832 [00:00<00:00, 17344.33it/s]\n",
      "splitting into MaxStep: 100%|██████████| 3331/3331 [00:00<00:00, 16859.70it/s]\n",
      "splitting into MaxStep: 100%|██████████| 832/832 [00:00<00:00, 18053.83it/s]\n"
     ]
    }
   ],
   "source": [
    "allFoldSeq=np.load(basePath+'raw/allFoldSeq.npy',allow_pickle=True)\n",
    "MAX_STEP = 128\n",
    "i=1\n",
    "for train_sequences,test_sequences in allFoldSeq:\n",
    "    train_data=splitToMaxStep(train_sequences,MAX_STEP)\n",
    "    test_data=splitToMaxStep(test_sequences,MAX_STEP)\n",
    "    trainFile=open(basePath+'raw/train/train_data'+'_'+str(i)+'.txt','wb')\n",
    "    testFile=open(basePath+'raw/test/test_data'+'_'+str(i)+'.txt','wb')\n",
    "    pickle.dump(train_data,trainFile)\n",
    "    pickle.dump(test_data,testFile)\n",
    "    trainFile.close()\n",
    "    testFile.close()\n",
    "    i=i+1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hjlTorch1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "290091e372d03c8158b7bbd399fcd5d922307b87125be2eaecd4834ca6d9de07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
